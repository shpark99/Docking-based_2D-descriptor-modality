{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682dd1d7-9a56-4514-ab81-878861c10cc9",
   "metadata": {},
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568bfc1-6ce9-4e36-8e85-d60bce6253ea",
   "metadata": {},
   "source": [
    "## test data 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b2134-32a7-44f5-b317-7d08f5e478e0",
   "metadata": {},
   "source": [
    "### Part 1: 디스크립터 계산 & with_desc 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081368d1-8e1e-4e17-920a-8fc26d817340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO][P1] Processing single file: /home/ssm-user/LAIDD/tox21/Data/test_data/test_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02:44:05] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 10\n",
      "[02:44:08] Can't kekulize mol.  Unkekulized atoms: 6 7 8 9 10 11 12 13 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[READY][P1] saved -> /home/ssm-user/LAIDD/tox21/Data/test_data/test_data_with_desc.csv (rows=645, cols=219)\n"
     ]
    }
   ],
   "source": [
    "# ===================== Part 1: 디스크립터 계산 & with_desc 저장 (단일 입력 전용) =====================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 단일 입력/출력 경로\n",
    "DATA_FILE = \"/home/ssm-user/LAIDD/tox21/Data/test_data/test_data.csv\"\n",
    "OUT_DIR   = \"/home/ssm-user/LAIDD/tox21/Data/test_data/\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# RDKit 2D descriptor 유틸\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "try:\n",
    "    from rdkit.Chem import Descriptors3D\n",
    "    desc3d = {n for n, _ in Descriptors3D._descList}\n",
    "except Exception:\n",
    "    desc3d = set()\n",
    "\n",
    "DESC_2D_NAMES = [n for n, _ in Descriptors._descList if n not in desc3d]\n",
    "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
    "_calc = MolecularDescriptorCalculator(DESC_2D_NAMES)\n",
    "\n",
    "def rdkit_2d_descriptors_from_series(smiles_series: pd.Series,\n",
    "                                     keep_all_rows: bool = True) -> pd.DataFrame:\n",
    "    rows, idxs = [], []\n",
    "    for idx, smi in smiles_series.items():\n",
    "        smi = \"\" if pd.isna(smi) else str(smi).strip()\n",
    "        if not smi:\n",
    "            if keep_all_rows:\n",
    "                rows.append([np.nan] * len(DESC_2D_NAMES)); idxs.append(idx)\n",
    "            continue\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            if keep_all_rows:\n",
    "                rows.append([np.nan] * len(DESC_2D_NAMES)); idxs.append(idx)\n",
    "            continue\n",
    "        try:\n",
    "            vals = list(_calc.CalcDescriptors(mol))\n",
    "        except Exception:\n",
    "            vals = [np.nan] * len(DESC_2D_NAMES)\n",
    "        rows.append(vals); idxs.append(idx)\n",
    "    return pd.DataFrame(rows, columns=DESC_2D_NAMES, index=idxs)\n",
    "\n",
    "def add_rdkit_2d_descriptors(df: pd.DataFrame,\n",
    "                             smiles_col: str = \"SMILES\",\n",
    "                             keep_all_rows: bool = False) -> pd.DataFrame:\n",
    "    desc_df = rdkit_2d_descriptors_from_series(df[smiles_col], keep_all_rows=keep_all_rows)\n",
    "    # keep_all_rows=False → RDKit 계산 성공한 행만 남도록 inner-join 효과\n",
    "    return df.join(desc_df, how=\"inner\") if not keep_all_rows else pd.concat([df, desc_df.reindex(df.index)], axis=1)\n",
    "\n",
    "# --- 메인 (단일 파일만 처리) ---\n",
    "assay_name = os.path.splitext(os.path.basename(DATA_FILE))[0]\n",
    "print(f\"\\n[INFO][P1] Processing single file: {DATA_FILE}\")\n",
    "\n",
    "# 1) 로드\n",
    "df = pd.read_csv(DATA_FILE, dtype=str, engine=\"python\")\n",
    "\n",
    "# 2) 필수 컬럼 확인 (SMILES, Sample ID만 필요)\n",
    "required = {\"SMILES\", \"Sample ID\"}\n",
    "if not required.issubset(df.columns):\n",
    "    raise ValueError(f\"Input must have SMILES and Sample ID columns. Found: {list(df.columns)}\")\n",
    "\n",
    "# 3) 문자열 정리 + 빈 SMILES 제거(로그용)\n",
    "df[\"SMILES\"] = df[\"SMILES\"].astype(str).str.strip()\n",
    "df[\"Sample ID\"] = df[\"Sample ID\"].astype(str).str.strip()\n",
    "before = len(df)\n",
    "df = df[df[\"SMILES\"].str.len() > 0]\n",
    "removed_empty = before - len(df)\n",
    "if removed_empty > 0:\n",
    "    print(f\"  -> Removed {removed_empty} rows with empty SMILES.\")\n",
    "\n",
    "# 4) 디스크립터 계산 (유효 SMILES만)\n",
    "df_with_desc = add_rdkit_2d_descriptors(df, smiles_col=\"SMILES\", keep_all_rows=False)\n",
    "\n",
    "# 5) 저장: 정제 없이 그대로 저장\n",
    "out_path = os.path.join(OUT_DIR, f\"{assay_name}_with_desc.csv\")\n",
    "df_with_desc.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"[READY][P1] saved -> {out_path} (rows={len(df_with_desc)}, cols={len(df_with_desc.columns)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a2817-94c1-4b2b-ac02-db0ebf35465e",
   "metadata": {},
   "source": [
    "### Part 2: _with_desc.csv 전처리 후 _2Ddesc.csv 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ad3206-d30c-491b-8f73-567fde67a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO][P2-Test] Load with_desc: /home/ssm-user/LAIDD/tox21/Data/test_data/test_data_with_desc.csv\n",
      "[P2-Test] Loaded caps: 205 features from /home/ssm-user/LAIDD/tox21/train_data_inf/clip_caps.json\n",
      "  -> Drop common-NaN columns first: 12\n",
      "  -> No columns contain NaN after capping.\n",
      "[READY][P2-Test] saved -> /home/ssm-user/LAIDD/tox21/Data/test_data/test_data_processed.csv (rows=645, cols=207) | cap_from_train=True, NaN columns dropped=0\n"
     ]
    }
   ],
   "source": [
    "# ====== Part 2 (TEST용, 단일 파일): _with_desc.csv 전처리 후 _2Ddesc.csv 저장\n",
    "#  - 흐름: 공통 NaN 컬럼 선삭제 → (train에서 저장한) cap으로 INF/값 클립 → NaN 포함 열 삭제\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 입력/출력 경로 ---\n",
    "OUT_DIR         = \"/home/ssm-user/LAIDD/tox21/Data/test_data/\"\n",
    "WITH_DESC_FILE  = os.path.join(OUT_DIR, \"test_data_with_desc.csv\")\n",
    "SAVE_BASE       = os.path.join(OUT_DIR, \"test_data_processed\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 공통 NaN 컬럼 드롭 리스트 (CSV만 사용) ---\n",
    "DROP_LIST_CSV   = \"/home/ssm-user/LAIDD/tox21/train_data_inf/data_summary/columns_with_nan_in_all_12_assays.csv\"\n",
    "\n",
    "# --- train에서 저장해 둔 컬럼별 cap 경로 ---\n",
    "CLIP_CAPS_JSON  = \"/home/ssm-user/LAIDD/tox21/train_data_inf/clip_caps.json\"\n",
    "\n",
    "def load_common_nan_columns_from_csv(csv_path: str) -> set[str]:\n",
    "    cols = set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[P2-Test] Drop-list CSV not found: {csv_path} (skip)\")\n",
    "        return cols\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"column\" in df.columns:\n",
    "            cols.update(df[\"column\"].dropna().astype(str).str.strip().tolist())\n",
    "        else:\n",
    "            first_col = df.columns[0]\n",
    "            cols.update(df[first_col].dropna().astype(str).str.strip().tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read CSV drop list: {csv_path} ({e})\")\n",
    "    return cols\n",
    "\n",
    "def load_clip_caps(json_path: str) -> dict:\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"[P2-Test] clip_caps.json not found: {json_path}\")\n",
    "    with open(json_path, \"r\") as f:\n",
    "        caps = json.load(f)\n",
    "    clean = {}\n",
    "    for k, v in caps.items():\n",
    "        try:\n",
    "            clean[str(k)] = float(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(f\"[P2-Test] Loaded caps: {len(clean)} features from {json_path}\")\n",
    "    return clean\n",
    "\n",
    "print(f\"\\n[INFO][P2-Test] Load with_desc: {WITH_DESC_FILE}\")\n",
    "df = pd.read_csv(WITH_DESC_FILE, dtype=str, engine=\"python\")\n",
    "\n",
    "required = {\"SMILES\", \"Sample ID\"}\n",
    "if not required.issubset(df.columns):\n",
    "    raise ValueError(f\"with_desc must have SMILES and Sample ID. Found: {list(df.columns)}\")\n",
    "\n",
    "df[\"SMILES\"]    = df[\"SMILES\"].astype(str).str.strip()\n",
    "df[\"Sample ID\"] = df[\"Sample ID\"].astype(str).str.strip()\n",
    "\n",
    "COMMON_NAN_COLS = load_common_nan_columns_from_csv(DROP_LIST_CSV)\n",
    "CLIP_CAPS       = load_clip_caps(CLIP_CAPS_JSON)\n",
    "\n",
    "non_feat  = {\"Sample ID\", \"SMILES\"}\n",
    "feat_cols = [c for c in df.columns if c not in non_feat]\n",
    "if COMMON_NAN_COLS:\n",
    "    drop_cols_common = sorted(set(feat_cols).intersection(COMMON_NAN_COLS))\n",
    "    if drop_cols_common:\n",
    "        print(f\"  -> Drop common-NaN columns first: {len(drop_cols_common)}\")\n",
    "        df = df.drop(columns=drop_cols_common, errors=\"ignore\")\n",
    "        feat_cols = [c for c in feat_cols if c not in drop_cols_common]\n",
    "    else:\n",
    "        print(\"  -> No common-NaN columns to drop in this file.\")\n",
    "else:\n",
    "    print(\"  -> No common-NaN drop list loaded or list is empty.\")\n",
    "\n",
    "# (수정) 숫자화 후 float64로 통일\n",
    "X = df[feat_cols].apply(pd.to_numeric, errors=\"coerce\").astype(np.float64)\n",
    "\n",
    "# train에서 cap이 정의된 컬럼만 유지\n",
    "caps_cols = sorted(set(feat_cols).intersection(CLIP_CAPS.keys()))\n",
    "unused_cols = sorted(set(feat_cols) - set(caps_cols))\n",
    "if unused_cols:\n",
    "    print(f\"  -> Drop columns without train caps ({len(unused_cols)}): \"\n",
    "          f\"{unused_cols[:5]}{'...' if len(unused_cols) > 5 else ''}\")\n",
    "X = X[caps_cols]\n",
    "\n",
    "# cap 적용: ±inf 대체 + inf값만 clip\n",
    "for col in caps_cols:\n",
    "    cap = CLIP_CAPS[col]\n",
    "    arr = X[col].to_numpy(copy=False)  # float64\n",
    "    # ±inf만 cap으로 바꿔치기\n",
    "    np.copyto(arr, +cap, where=np.isposinf(arr))\n",
    "    np.copyto(arr, -cap, where=np.isneginf(arr))\n",
    "    # np.clip(...) 제거!  유한값은 손대지 않음\n",
    "    X[col] = arr\n",
    "\n",
    "# NaN 포함 열 전부 삭제\n",
    "nan_cols = X.columns[X.isna().any(axis=0)].tolist()\n",
    "if nan_cols:\n",
    "    print(f\"  -> Drop columns containing NaN ({len(nan_cols)}): \"\n",
    "          f\"{nan_cols[:5]}{'...' if len(nan_cols) > 5 else ''}\")\n",
    "    X = X.drop(columns=nan_cols)\n",
    "else:\n",
    "    print(\"  -> No columns contain NaN after capping.\")\n",
    "\n",
    "if X.shape[1] == 0:\n",
    "    raise RuntimeError(\"No feature columns remain after dropping NaN columns. Check preprocessing settings.\")\n",
    "\n",
    "# 비유한값 최종 점검 (NaN/±inf 모두 없어야 함)\n",
    "assert np.isfinite(X.values).all(), \"Processed X still has non-finite values (NaN or ±inf).\"\n",
    "\n",
    "left_part = df.loc[:, [\"Sample ID\", \"SMILES\"]].reset_index(drop=True)\n",
    "X_final   = X.reset_index(drop=True)\n",
    "df_save   = pd.concat([left_part, X_final], axis=1)\n",
    "df_save.to_csv(SAVE_BASE + \".csv\", index=False)\n",
    "\n",
    "print(f\"[READY][P2-Test] saved -> {SAVE_BASE+'.csv'} \"\n",
    "      f\"(rows={len(df_save)}, cols={len(df_save.columns)}) | \"\n",
    "      f\"cap_from_train=True, NaN columns dropped={len(nan_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52fb0a40-a133-4dd5-8529-c770e246e22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgIpc</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi0n</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi1n</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>Chi2n</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "      <th>qed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.936855</td>\n",
       "      <td>4.908470e-07</td>\n",
       "      <td>1682.841056</td>\n",
       "      <td>37.329487</td>\n",
       "      <td>31.212127</td>\n",
       "      <td>31.212127</td>\n",
       "      <td>25.440364</td>\n",
       "      <td>19.621438</td>\n",
       "      <td>19.621438</td>\n",
       "      <td>16.370604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.969295</td>\n",
       "      <td>1.689999e-06</td>\n",
       "      <td>837.416772</td>\n",
       "      <td>16.353007</td>\n",
       "      <td>12.273696</td>\n",
       "      <td>16.754120</td>\n",
       "      <td>10.871811</td>\n",
       "      <td>6.486421</td>\n",
       "      <td>8.740012</td>\n",
       "      <td>5.050654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.942977</td>\n",
       "      <td>1.927142e+00</td>\n",
       "      <td>1104.420013</td>\n",
       "      <td>14.681434</td>\n",
       "      <td>11.835691</td>\n",
       "      <td>11.835691</td>\n",
       "      <td>10.826500</td>\n",
       "      <td>7.423033</td>\n",
       "      <td>7.423033</td>\n",
       "      <td>5.683487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.106280</td>\n",
       "      <td>2.253331e-06</td>\n",
       "      <td>700.609974</td>\n",
       "      <td>16.777810</td>\n",
       "      <td>13.491812</td>\n",
       "      <td>14.308309</td>\n",
       "      <td>11.720347</td>\n",
       "      <td>7.823811</td>\n",
       "      <td>7.823811</td>\n",
       "      <td>5.518027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.544034</td>\n",
       "      <td>2.227488e+00</td>\n",
       "      <td>758.185306</td>\n",
       "      <td>16.872033</td>\n",
       "      <td>12.698674</td>\n",
       "      <td>14.271099</td>\n",
       "      <td>10.919852</td>\n",
       "      <td>7.137723</td>\n",
       "      <td>9.155409</td>\n",
       "      <td>5.276592</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.779959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1.405639</td>\n",
       "      <td>2.190610e+00</td>\n",
       "      <td>14.364528</td>\n",
       "      <td>4.121320</td>\n",
       "      <td>3.015748</td>\n",
       "      <td>3.910175</td>\n",
       "      <td>2.414214</td>\n",
       "      <td>1.632456</td>\n",
       "      <td>2.264911</td>\n",
       "      <td>0.800767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>2.185557</td>\n",
       "      <td>2.399922e+00</td>\n",
       "      <td>136.152234</td>\n",
       "      <td>11.096012</td>\n",
       "      <td>9.047243</td>\n",
       "      <td>9.047243</td>\n",
       "      <td>7.295555</td>\n",
       "      <td>5.294733</td>\n",
       "      <td>5.294733</td>\n",
       "      <td>3.778366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>1.905400</td>\n",
       "      <td>3.391829e+00</td>\n",
       "      <td>197.454754</td>\n",
       "      <td>5.276021</td>\n",
       "      <td>3.749889</td>\n",
       "      <td>4.566386</td>\n",
       "      <td>3.304530</td>\n",
       "      <td>1.745765</td>\n",
       "      <td>2.154013</td>\n",
       "      <td>1.141591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.480265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>3.086707</td>\n",
       "      <td>1.928025e+00</td>\n",
       "      <td>709.994725</td>\n",
       "      <td>15.087576</td>\n",
       "      <td>11.220996</td>\n",
       "      <td>13.549351</td>\n",
       "      <td>9.935071</td>\n",
       "      <td>6.684735</td>\n",
       "      <td>7.848912</td>\n",
       "      <td>5.521094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.646146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>2.139133</td>\n",
       "      <td>2.762608e+00</td>\n",
       "      <td>122.636094</td>\n",
       "      <td>5.112884</td>\n",
       "      <td>3.794619</td>\n",
       "      <td>4.689047</td>\n",
       "      <td>3.431852</td>\n",
       "      <td>2.064949</td>\n",
       "      <td>2.697405</td>\n",
       "      <td>1.237477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>645 rows × 205 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AvgIpc      BalabanJ      BertzCT       Chi0      Chi0n      Chi0v  \\\n",
       "0    3.936855  4.908470e-07  1682.841056  37.329487  31.212127  31.212127   \n",
       "1    2.969295  1.689999e-06   837.416772  16.353007  12.273696  16.754120   \n",
       "2    2.942977  1.927142e+00  1104.420013  14.681434  11.835691  11.835691   \n",
       "3    3.106280  2.253331e-06   700.609974  16.777810  13.491812  14.308309   \n",
       "4    2.544034  2.227488e+00   758.185306  16.872033  12.698674  14.271099   \n",
       "..        ...           ...          ...        ...        ...        ...   \n",
       "640  1.405639  2.190610e+00    14.364528   4.121320   3.015748   3.910175   \n",
       "641  2.185557  2.399922e+00   136.152234  11.096012   9.047243   9.047243   \n",
       "642  1.905400  3.391829e+00   197.454754   5.276021   3.749889   4.566386   \n",
       "643  3.086707  1.928025e+00   709.994725  15.087576  11.220996  13.549351   \n",
       "644  2.139133  2.762608e+00   122.636094   5.112884   3.794619   4.689047   \n",
       "\n",
       "          Chi1      Chi1n      Chi1v      Chi2n  ...  fr_sulfonamd  \\\n",
       "0    25.440364  19.621438  19.621438  16.370604  ...           0.0   \n",
       "1    10.871811   6.486421   8.740012   5.050654  ...           0.0   \n",
       "2    10.826500   7.423033   7.423033   5.683487  ...           0.0   \n",
       "3    11.720347   7.823811   7.823811   5.518027  ...           0.0   \n",
       "4    10.919852   7.137723   9.155409   5.276592  ...           1.0   \n",
       "..         ...        ...        ...        ...  ...           ...   \n",
       "640   2.414214   1.632456   2.264911   0.800767  ...           0.0   \n",
       "641   7.295555   5.294733   5.294733   3.778366  ...           0.0   \n",
       "642   3.304530   1.745765   2.154013   1.141591  ...           0.0   \n",
       "643   9.935071   6.684735   7.848912   5.521094  ...           0.0   \n",
       "644   3.431852   2.064949   2.697405   1.237477  ...           0.0   \n",
       "\n",
       "     fr_sulfone  fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  \\\n",
       "0           0.0                0.0           0.0          0.0          0.0   \n",
       "1           0.0                0.0           0.0          0.0          0.0   \n",
       "2           0.0                0.0           0.0          0.0          0.0   \n",
       "3           0.0                0.0           0.0          0.0          0.0   \n",
       "4           0.0                0.0           0.0          0.0          0.0   \n",
       "..          ...                ...           ...          ...          ...   \n",
       "640         0.0                0.0           0.0          0.0          0.0   \n",
       "641         0.0                0.0           0.0          0.0          0.0   \n",
       "642         0.0                0.0           1.0          0.0          0.0   \n",
       "643         0.0                0.0           0.0          0.0          0.0   \n",
       "644         0.0                0.0           0.0          0.0          0.0   \n",
       "\n",
       "     fr_thiophene  fr_unbrch_alkane  fr_urea       qed  \n",
       "0             0.0               0.0      0.0  0.276843  \n",
       "1             0.0               0.0      0.0  0.262347  \n",
       "2             0.0               0.0      0.0  0.540057  \n",
       "3             0.0               0.0      0.0  0.608509  \n",
       "4             0.0               0.0      0.0  0.779959  \n",
       "..            ...               ...      ...       ...  \n",
       "640           0.0               0.0      0.0  0.469276  \n",
       "641           0.0               0.0      0.0  0.468960  \n",
       "642           0.0               0.0      0.0  0.480265  \n",
       "643           0.0               0.0      0.0  0.646146  \n",
       "644           0.0               0.0      0.0  0.548346  \n",
       "\n",
       "[645 rows x 205 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ffd8bac-416d-4c5a-8a4b-683392c19051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO][P2-Test] Load with_desc: /home/ssm-user/LAIDD/tox21/Data/test_data/test_data_with_desc.csv\n",
      "  -> Drop common-NaN columns first: 12\n",
      "  -> No columns contain NaN.\n",
      "[READY][P2-Test] saved -> /home/ssm-user/LAIDD/tox21/Data/test_data/test_data_processed.csv (rows=645, cols=207) | NaN columns dropped=0\n"
     ]
    }
   ],
   "source": [
    "# ====== Part 2 (TEST용, 단일 파일): _with_desc.csv 전처리 후 _2Ddesc.csv 저장\n",
    "#  - 흐름: 공통 NaN 컬럼 선삭제 → NaN 포함 열 삭제\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 입력/출력 경로 ---\n",
    "OUT_DIR         = \"/home/ssm-user/LAIDD/tox21/Data/test_data/\"\n",
    "WITH_DESC_FILE  = os.path.join(OUT_DIR, \"test_data_with_desc.csv\")\n",
    "SAVE_BASE       = os.path.join(OUT_DIR, \"test_data_processed\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 공통 NaN 컬럼 드롭 리스트 (CSV만 사용) ---\n",
    "DROP_LIST_CSV   = \"/home/ssm-user/LAIDD/tox21/train_data_inf/data_summary/columns_with_nan_in_all_12_assays.csv\"\n",
    "\n",
    "def load_common_nan_columns_from_csv(csv_path: str) -> set[str]:\n",
    "    cols = set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[P2-Test] Drop-list CSV not found: {csv_path} (skip)\")\n",
    "        return cols\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"column\" in df.columns:\n",
    "            cols.update(df[\"column\"].dropna().astype(str).str.strip().tolist())\n",
    "        else:\n",
    "            first_col = df.columns[0]\n",
    "            cols.update(df[first_col].dropna().astype(str).str.strip().tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read CSV drop list: {csv_path} ({e})\")\n",
    "    return cols\n",
    "\n",
    "print(f\"\\n[INFO][P2-Test] Load with_desc: {WITH_DESC_FILE}\")\n",
    "df = pd.read_csv(WITH_DESC_FILE, dtype=str, engine=\"python\")\n",
    "\n",
    "required = {\"SMILES\", \"Sample ID\"}\n",
    "if not required.issubset(df.columns):\n",
    "    raise ValueError(f\"with_desc must have SMILES and Sample ID. Found: {list(df.columns)}\")\n",
    "\n",
    "df[\"SMILES\"]    = df[\"SMILES\"].astype(str).str.strip()\n",
    "df[\"Sample ID\"] = df[\"Sample ID\"].astype(str).str.strip()\n",
    "\n",
    "COMMON_NAN_COLS = load_common_nan_columns_from_csv(DROP_LIST_CSV)\n",
    "\n",
    "non_feat  = {\"Sample ID\", \"SMILES\"}\n",
    "feat_cols = [c for c in df.columns if c not in non_feat]\n",
    "if COMMON_NAN_COLS:\n",
    "    drop_cols_common = sorted(set(feat_cols).intersection(COMMON_NAN_COLS))\n",
    "    if drop_cols_common:\n",
    "        print(f\"  -> Drop common-NaN columns first: {len(drop_cols_common)}\")\n",
    "        df = df.drop(columns=drop_cols_common, errors=\"ignore\")\n",
    "        feat_cols = [c for c in feat_cols if c not in drop_cols_common]\n",
    "    else:\n",
    "        print(\"  -> No common-NaN columns to drop in this file.\")\n",
    "else:\n",
    "    print(\"  -> No common-NaN drop list loaded or list is empty.\")\n",
    "\n",
    "# (수정) 숫자화 후 float64로 통일\n",
    "X = df[feat_cols].apply(pd.to_numeric, errors=\"coerce\").astype(np.float64)\n",
    "\n",
    "# NaN 포함 열 전부 삭제\n",
    "nan_cols = X.columns[X.isna().any(axis=0)].tolist()\n",
    "if nan_cols:\n",
    "    print(f\"  -> Drop columns containing NaN ({len(nan_cols)}): \"\n",
    "          f\"{nan_cols[:5]}{'...' if len(nan_cols) > 5 else ''}\")\n",
    "    X = X.drop(columns=nan_cols)\n",
    "else:\n",
    "    print(\"  -> No columns contain NaN.\")\n",
    "\n",
    "if X.shape[1] == 0:\n",
    "    raise RuntimeError(\"No feature columns remain after dropping NaN columns. Check preprocessing settings.\")\n",
    "\n",
    "# 비유한값 최종 점검 (NaN/±inf 모두 없어야 함)\n",
    "assert np.isfinite(X.values).all(), \"Processed X still has non-finite values (NaN or ±inf).\"\n",
    "\n",
    "left_part = df.loc[:, [\"Sample ID\", \"SMILES\"]].reset_index(drop=True)\n",
    "X_final   = X.reset_index(drop=True)\n",
    "df_save   = pd.concat([left_part, X_final], axis=1)\n",
    "df_save.to_csv(SAVE_BASE + \".csv\", index=False)\n",
    "\n",
    "print(f\"[READY][P2-Test] saved -> {SAVE_BASE+'.csv'} \"\n",
    "      f\"(rows={len(df_save)}, cols={len(df_save.columns)}) | \"\n",
    "      f\"NaN columns dropped={len(nan_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3675d6c5-4ffd-4d7a-b054-c521bb68bbeb",
   "metadata": {},
   "source": [
    "### Part 3: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e95508-a7f9-43e1-bb3f-110bbbbb1089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Loading model & assays ...\n",
      "[TEST] #features expected by model: 205\n",
      "[TEST] Model internal n_features_in_: 205\n",
      "[TEST] Loading test data: /home/ssm-user/LAIDD/tox21/Data/test_data/test_data_processed.csv\n",
      "[TEST] Predicting probabilities ...\n",
      "[READY] Saved stacking-ready test probabilities -> /home/ssm-user/LAIDD/tox21/Results_imputer_inf/2D_predictions.csv\n",
      "        rows=645, assays=12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# ===================== 경로 설정 =====================\n",
    "MODEL_DIR   = \"/home/ssm-user/LAIDD/tox21/Results_imputer_inf\"\n",
    "MODEL_PATH  = os.path.join(MODEL_DIR, \"multilabel_rf_best.joblib\")\n",
    "ASSAYS_PATH = os.path.join(MODEL_DIR, \"assays.json\")\n",
    "\n",
    "TEST_PATH   = \"/home/ssm-user/LAIDD/tox21/Data/test_data/test_data_processed.csv\"  # 이미 전처리 완료본\n",
    "TEST_OUT    = os.path.join(MODEL_DIR, \"2D_predictions.csv\")                  # 스태킹용 출력\n",
    "\n",
    "# ===================== 유틸 =====================\n",
    "def load_assays(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"assays.json not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        assays = json.load(f)\n",
    "    if not isinstance(assays, list) or not assays:\n",
    "        raise ValueError(\"assays.json is not a non-empty list.\")\n",
    "    return assays\n",
    "\n",
    "def get_training_feature_names(pipe) -> list:\n",
    "    \"\"\"\n",
    "    학습 시 사용된 피처명(순서 포함)을 파이프라인에서 복구.\n",
    "    - 1순위: 첫 스텝(SimpleImputer 등)의 feature_names_in_\n",
    "    - 2순위: 파이프라인 자체의 feature_names_in_\n",
    "    \"\"\"\n",
    "    if hasattr(pipe, \"named_steps\") and \"imp\" in pipe.named_steps:\n",
    "        imp = pipe.named_steps[\"imp\"]\n",
    "        if hasattr(imp, \"feature_names_in_\"):\n",
    "            return list(imp.feature_names_in_)\n",
    "    if hasattr(pipe, \"feature_names_in_\"):\n",
    "        return list(pipe.feature_names_in_)\n",
    "    raise RuntimeError(\n",
    "        \"Cannot find training feature names in the fitted pipeline. \"\n",
    "        \"Ensure the model was trained with a pandas DataFrame so steps expose 'feature_names_in_'.\"\n",
    "    )\n",
    "\n",
    "# ===================== 메인 =====================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[TEST] Loading model & assays ...\")\n",
    "    pipe = joblib.load(MODEL_PATH)                       # Pipeline(SimpleImputer + MultiOutput(RandomForest))\n",
    "    assays = load_assays(ASSAYS_PATH)\n",
    "    expected_features = get_training_feature_names(pipe)\n",
    "    print(f\"[TEST] #features expected by model: {len(expected_features)}\")\n",
    "\n",
    "    # (선택) 내부 차원 로그\n",
    "    try:\n",
    "        n_expected_internal = pipe.named_steps[\"clf\"].estimators_[0].n_features_in_\n",
    "        print(f\"[TEST] Model internal n_features_in_: {n_expected_internal}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[TEST] Loading test data: {TEST_PATH}\")\n",
    "    test_df = pd.read_csv(TEST_PATH, low_memory=False,\n",
    "                          dtype={\"Sample ID\":\"string\",\"SMILES\":\"string\"})\n",
    "    # 키 정리\n",
    "    test_df[\"Sample ID\"] = test_df[\"Sample ID\"].astype(\"string\").str.strip()\n",
    "    test_df[\"SMILES\"]    = test_df[\"SMILES\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # 피처 부분만 분리\n",
    "    non_feat = {\"Sample ID\", \"SMILES\", \"toxicity\"}\n",
    "    test_feat_cols = [c for c in test_df.columns if c not in non_feat]\n",
    "    X_test = test_df[test_feat_cols].copy()\n",
    "\n",
    "    # 수치 변환 + 안전장치(±inf -> NaN)\n",
    "    X_test = X_test.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).astype(\"float64\")\n",
    "\n",
    "    # 학습 스키마로 정렬: 누락 컬럼은 자동으로 NaN 생성, 여분 컬럼은 드롭\n",
    "    exp_set = set(expected_features)\n",
    "    cur_set = set(X_test.columns)\n",
    "    missing = sorted(exp_set - cur_set)\n",
    "    extra   = sorted(cur_set - exp_set)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"[TEST][WARN] Missing {len(missing)} feature(s) in test \"\n",
    "              f\"(filled as NaN). e.g., {missing[:5]}{'...' if len(missing)>5 else ''}\")\n",
    "    if extra:\n",
    "        print(f\"[TEST] Dropping {len(extra)} unexpected test feature(s). \"\n",
    "              f\"e.g., {extra[:5]}{'...' if len(extra)>5 else ''}\")\n",
    "\n",
    "    X_test = X_test.reindex(columns=expected_features)\n",
    "\n",
    "    assert list(X_test.columns) == list(expected_features)  # 학습 피처와 완전 동일\n",
    "\n",
    "    # ===== 예측 (파이프라인 전체 호출: imputer -> classifier) =====\n",
    "    print(\"[TEST] Predicting probabilities ...\")\n",
    "    proba_out = pipe.predict_proba(X_test)  # MultiOutputClassifier: list of (n,2) or ndarray (n,2) when single\n",
    "    proba_list = proba_out if isinstance(proba_out, list) else [proba_out]\n",
    "    prob_mat = np.column_stack([p[:, 1] for p in proba_list]).astype(np.float32)\n",
    "\n",
    "    # ===== 저장: Sample ID, SMILES + assay별 확률 =====\n",
    "    out_df = pd.concat(\n",
    "        [test_df.loc[:, [\"Sample ID\", \"SMILES\"]].reset_index(drop=True),\n",
    "         pd.DataFrame(prob_mat, columns=assays)],\n",
    "        axis=1\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(TEST_OUT), exist_ok=True)\n",
    "    out_df.to_csv(TEST_OUT, index=False)\n",
    "    print(f\"[READY] Saved stacking-ready test probabilities -> {TEST_OUT}\")\n",
    "    print(f\"        rows={len(out_df)}, assays={len(assays)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8579c-3b9a-4ef0-b5c2-6835784dfc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e88e6-8d31-4000-bc19-3722d1dcafce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
